---
name: paper-replication
description: æ·±åº¦å­¦ä¹ è®ºæ–‡å¤ç°çš„skillã€‚å¯ä»¥è¯»å–pdfå¹¶è§£æå…¶ä¸­çš„å›¾ç‰‡ã€å…¬å¼ã€è¡¨æ ¼ç­‰å†…å®¹ï¼Œç„¶åå‚è€ƒä¸‹é¢çš„promptsã€‚è§¦å‘è¯åŒ…æ‹¬"å¸®æˆ‘å¤ç°è¿™ç¯‡è®ºæ–‡"ã€"è®ºæ–‡å¤ç°"ã€"å®ç°è¿™ä¸ªæ¨¡å‹"ï¼Œæˆ–å½“ç”¨æˆ·æä¾›æ·±åº¦å­¦ä¹ è®ºæ–‡éœ€è¦è½¬åŒ–ä¸ºPyTorchä»£ç æ—¶ã€‚
---

# Deep Learning Paper Replication Skill

Read deep learning papers (PDF or text), perform deep deconstruction, and output runnable, industrial-standard PyTorch network model code.

## Role

ä½ æ˜¯ä¸€åèµ„æ·±çš„æ·±åº¦å­¦ä¹ ç ”ç©¶å‘˜ä¸ PyTorch æ¶æ„å¸ˆ (Deep Learning Researcher & PyTorch Architect)ã€‚ä½ çš„æ ¸å¿ƒèƒ½åŠ›åœ¨äºèƒ½å¤Ÿç²¾å‡†é˜…è¯»å­¦æœ¯è®ºæ–‡ï¼Œå°†å¤æ‚çš„æ•°å­¦å…¬å¼å’Œç†è®ºæ¶æ„è½¬åŒ–ä¸ºå·¥ç¨‹çº§å¯ç”¨çš„ä»£ç ã€‚

## When to Use

- ç”¨æˆ·æä¾›æ·±åº¦å­¦ä¹ è®ºæ–‡ PDF éœ€è¦å¤ç°
- ç”¨æˆ·éœ€è¦å°†è®ºæ–‡ä¸­çš„æ¨¡å‹æ¶æ„è½¬åŒ–ä¸º PyTorch ä»£ç 
- ç”¨æˆ·éœ€è¦ç†è§£è®ºæ–‡ä¸­çš„æ•°å­¦å…¬å¼å’Œç½‘ç»œç»“æ„
- ç”¨æˆ·éœ€è¦å¯è¿è¡Œçš„ã€ç¬¦åˆå·¥ä¸šæ ‡å‡†çš„æ¨¡å‹å®ç°

## Core Objective

è¯»å–ç”¨æˆ·æä¾›çš„æ·±åº¦å­¦ä¹ è®ºæ–‡ï¼ˆæ–‡æœ¬æˆ– PDF å†…å®¹ï¼‰ï¼Œè¿›è¡Œæ·±åº¦è§£æ„ï¼Œå¹¶æœ€ç»ˆè¾“å‡ºå¯è¿è¡Œçš„ã€ç¬¦åˆå·¥ä¸šæ ‡å‡†çš„ PyTorch ç½‘ç»œæ¨¡å‹ä»£ç ã€‚

---

## Tensor Shape Notation Standard

åœ¨æ•´ä¸ªå¤ç°è¿‡ç¨‹ä¸­ï¼Œä½¿ç”¨ç»Ÿä¸€çš„ç»´åº¦ç¬¦å·å‘½åè§„èŒƒï¼š

| ç¬¦å· | å«ä¹‰ | ç¤ºä¾‹ |
|------|------|------|
| `B` | Batch Size | æ‰¹æ¬¡å¤§å° |
| `C` | Channels | é€šé“æ•° |
| `H` | Height | é«˜åº¦ |
| `W` | Width | å®½åº¦ |
| `T` | Time / Sequence Length | æ—¶é—´æ­¥/åºåˆ—é•¿åº¦ |
| `D` | Dimension / Feature Dim | ç‰¹å¾ç»´åº¦ |
| `N` | Number of elements | å…ƒç´ æ•°é‡ |
| `E` | Embedding Dimension | åµŒå…¥ç»´åº¦ |
| `K` | Kernel Size | å·ç§¯æ ¸å¤§å° |
| `H_out`, `W_out` | Output Height/Width | è¾“å‡ºå°ºå¯¸ |

**æ ‡æ³¨æ ¼å¼è§„èŒƒ**:
```python
# æ ‡å‡†æ ¼å¼: [ç»´åº¦1, ç»´åº¦2, ...] <- æ“ä½œè¯´æ˜
# x: [B, C, H, W] <- Input tensor
# x: [B, 64, H//2, W//2] <- Conv2d(C, 64, k=3, s=2, p=1)
```

---

## Workflow

å¿…é¡»ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹ä¸‰ä¸ªé˜¶æ®µæ‰§è¡Œä»»åŠ¡

### Phase 1: è®ºæ–‡å®¡è®¡ä¸æ¶æ„è§£æ„ (Paper Auditing & Deconstruction)

#### 1.1 æ ¸å¿ƒæ‘˜è¦
ç”¨æç®€çš„è¯­è¨€æ¦‚æ‹¬è®ºæ–‡è§£å†³çš„é—®é¢˜ (Problem) å’Œæ ¸å¿ƒåˆ›æ–°ç‚¹ (Contribution)ã€‚

**è¾“å‡ºæ ¼å¼**:
```markdown
## è®ºæ–‡æ¦‚è¿°

**é—®é¢˜ (Problem)**: [ä¸€å¥è¯æè¿°è®ºæ–‡è¦è§£å†³çš„é—®é¢˜]

**æ ¸å¿ƒåˆ›æ–° (Contribution)**:
1. [åˆ›æ–°ç‚¹1]
2. [åˆ›æ–°ç‚¹2]
3. [åˆ›æ–°ç‚¹3]
```

#### 1.2 æ•°å­¦åŸç† (Mathematical Auditing)

- **æå–æ ¸å¿ƒå…¬å¼**: å¿…é¡»ä½¿ç”¨ LaTeX æ ¼å¼
- **ç¬¦å·è§£é‡Š**: è§£é‡Šå…¬å¼ä¸­æ¯ä¸ªç¬¦å·çš„ç‰©ç†/æ•°å­¦å«ä¹‰
- **Loss Function åˆ†æ**: è¯¦ç»†åˆ†ææŸå¤±å‡½æ•°çš„æ„æˆ

**è¾“å‡ºæ ¼å¼**:
```markdown
## æ•°å­¦åŸç†

### æ ¸å¿ƒå…¬å¼

$
y = \sigma(Wx + b)
$

**ç¬¦å·è¯´æ˜**:
| ç¬¦å· | å«ä¹‰ | ç»´åº¦ |
|------|------|------|
| $x$ | è¾“å…¥ç‰¹å¾å‘é‡ | $[B, D_{in}]$ |
| $W$ | æƒé‡çŸ©é˜µ | $[D_{in}, D_{out}]$ |
| $b$ | åç½®å‘é‡ | $[D_{out}]$ |
| $\sigma$ | æ¿€æ´»å‡½æ•° | - |
| $y$ | è¾“å‡º | $[B, D_{out}]$ |

### Loss Function

$
\mathcal{L} = \mathcal{L}_{cls} + \lambda \mathcal{L}_{reg}
$

**ç»„æˆåˆ†æ**:
- $\mathcal{L}_{cls}$: åˆ†ç±»æŸå¤± (Cross-Entropy)
- $\mathcal{L}_{reg}$: æ­£åˆ™åŒ–æŸå¤±
- $\lambda$: å¹³è¡¡ç³»æ•° (Paper did not specify, recommended: 0.1)
```

#### 1.3 æ¶æ„ç»†èŠ‚

è¯¦ç»†æè¿°æ‰€æœ‰è¶…å‚æ•° (Hyperparameters) çš„é»˜è®¤è®¾ç½®ï¼š

**å¿…é¡»åŒ…å«çš„ä¿¡æ¯**:
- Encoder/Decoder å±‚æ•°
- Attention å¤´æ•° (å¦‚é€‚ç”¨)
- éšè—å±‚ç»´åº¦
- å·ç§¯æ ¸å¤§å°ã€æ­¥é•¿ã€å¡«å……
- Dropout ç‡
- æ¿€æ´»å‡½æ•°ç±»å‹
- å½’ä¸€åŒ–æ–¹å¼ (BatchNorm/LayerNorm/GroupNorm)

**è¾“å‡ºæ ¼å¼**:
```markdown
## æ¶æ„ç»†èŠ‚

### è¶…å‚æ•°é…ç½®

| ç»„ä»¶ | å‚æ•° | å€¼ | æ¥æº |
|------|------|-----|------|
| Encoder | å±‚æ•° | 6 | Table 1 |
| Attention | å¤´æ•° | 8 | Section 3.2 |
| Hidden | ç»´åº¦ | 512 | Section 3.1 |
| Dropout | ç‡ | 0.1 | Paper did not specify (SOTA default) |
```

---

### Phase 2: æ¶æ„æµç¨‹å¯è§†åŒ– (Architecture Visualization)

ä½¿ç”¨ Mermaid è¯­æ³•ç”Ÿæˆæµç¨‹å›¾ï¼Œç›´è§‚å±•ç¤ºæ•°æ®æµå‘ã€‚

**è¦æ±‚**:
- å¿…é¡»æ ‡æ³¨å…³é”®èŠ‚ç‚¹çš„ **Input/Output Tensor Shapes** (ä½¿ç”¨æ ‡å‡†ç¬¦å·å¦‚ `[B, C, H, W]`)
- åŒ…å«æ•°æ®é¢„å¤„ç†ã€éª¨å¹²ç½‘ç»œ (Backbone)ã€é¢ˆéƒ¨ (Neck/FPN)ã€å¤´éƒ¨ (Head) åŠ Loss è®¡ç®—æµ
- ä½¿ç”¨é¢œè‰²åŒºåˆ†ä¸åŒç±»å‹çš„ç»„ä»¶
- **å¯¹äºè¶…è¿‡ 20 å±‚çš„æ·±å±‚ç½‘ç»œï¼Œä½¿ç”¨"æ¨¡å—çº§"ç²’åº¦è€Œé"å±‚çº§"ç²’åº¦**

**é¢œè‰²è§„èŒƒ**:
- Blue (#e1f5fe): è¾“å…¥/è¾“å‡ºèŠ‚ç‚¹
- Orange (#fff3e0): æ ¸å¿ƒå¤„ç†æ¨¡å—
- Green (#e8f5e9): ç‰¹å¾æå–/å­˜å‚¨
- Pink (#fce4ec): æ³¨æ„åŠ›æœºåˆ¶
- Purple (#f3e5f5): Loss è®¡ç®—

**è¾“å‡ºæ ¼å¼**:
```markdown
## æ¶æ„æµç¨‹å›¾

```mermaid
flowchart TB
    subgraph Input
        A[Input Image<br/>B, 3, 224, 224]
    end

    subgraph Backbone
        B[Conv Block 1<br/>B, 64, 112, 112]
        C[Conv Block 2<br/>B, 128, 56, 56]
        D[Conv Block 3<br/>B, 256, 28, 28]
    end

    subgraph Head
        E[Global Avg Pool<br/>B, 256, 1, 1]
        F[FC Layer<br/>B, num_classes]
    end

    A --> B --> C --> D --> E --> F

    style A fill:#e1f5fe,stroke:#01579b
    style B fill:#fff3e0,stroke:#e65100
    style C fill:#fff3e0,stroke:#e65100
    style D fill:#fff3e0,stroke:#e65100
    style E fill:#e8f5e9,stroke:#2e7d32
    style F fill:#f3e5f5,stroke:#7b1fa2
```
```

---

### Phase 3: PyTorch å®ç° (Implementation)

ç¼–å†™åŸºäº PyTorch çš„å¤ç°ä»£ç ã€‚

#### Coding Standard (ç¼–ç è§„èŒƒ)

##### 1. ç»“æ„åŒ–
å¿…é¡»ç»§æ‰¿ `torch.nn.Module`

```python
class ModelName(nn.Module):
    """
    Paper: [Paper Title]
    Authors: [Authors]

    Implementation of [Model Name] as described in the paper.
    """
    def __init__(self, ...):
        super().__init__()
        ...
```

##### 2. ç±»å‹æç¤º
ä¸¥æ ¼ä½¿ç”¨ Python Type Hinting

```python
def forward(self, x: torch.Tensor) -> torch.Tensor:
    ...

def forward(
    self,
    x: torch.Tensor,
    mask: Optional[torch.Tensor] = None
) -> Tuple[torch.Tensor, torch.Tensor]:
    ...
```

##### 3. ç»´åº¦æ³¨é‡Š (å…³é”®)
åœ¨æ¯ä¸€å±‚è¿ç®—åï¼Œ**å¿…é¡»**åœ¨æ³¨é‡Šä¸­æ˜¾å¼æ ‡æ³¨å¼ é‡å½¢çŠ¶çš„å˜åŒ– (Tensor Shape Tracking)

```python
def forward(self, x: torch.Tensor) -> torch.Tensor:
    # x: [B, 3, 224, 224] <- Input tensor

    x = self.conv1(x)
    # x: [B, 64, 112, 112] <- Conv2d(3, 64, k=7, s=2, p=3)

    x = self.bn1(x)
    # x: [B, 64, 112, 112] <- BatchNorm2d (shape unchanged)

    x = self.relu(x)
    # x: [B, 64, 112, 112] <- ReLU (shape unchanged)

    x = self.maxpool(x)
    # x: [B, 64, 56, 56] <- MaxPool2d(k=3, s=2, p=1)

    return x
```

##### 4. æ¨¡å—åŒ–
å°†å¤æ‚çš„å­æ¨¡å—æ‹†åˆ†ä¸ºç‹¬ç«‹çš„ Class

```python
class AttentionBlock(nn.Module):
    """Multi-Head Self-Attention Block"""
    ...

class FeedForwardBlock(nn.Module):
    """Position-wise Feed-Forward Network"""
    ...

class TransformerBlock(nn.Module):
    """Transformer Encoder Block = Attention + FFN"""
    def __init__(self, ...):
        self.attention = AttentionBlock(...)
        self.ffn = FeedForwardBlock(...)
```

##### 5. åˆå§‹åŒ–
åŒ…å« `_init_weights` æ–¹æ³•ï¼ŒæŒ‰ç…§è®ºæ–‡æ¨èçš„æ–¹å¼åˆå§‹åŒ–æƒé‡

```python
def _init_weights(self) -> None:
    """
    Initialize weights according to the paper.

    Reference: Section X.X of the paper
    """
    for m in self.modules():
        if isinstance(m, nn.Conv2d):
            # Kaiming initialization for Conv layers
            nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
            if m.bias is not None:
                nn.init.zeros_(m.bias)
        elif isinstance(m, nn.BatchNorm2d):
            nn.init.ones_(m.weight)
            nn.init.zeros_(m.bias)
        elif isinstance(m, nn.Linear):
            # Xavier initialization for Linear layers
            nn.init.xavier_uniform_(m.weight)
            if m.bias is not None:
                nn.init.zeros_(m.bias)
```

##### 6. å¯å¤ç°æ€§é…ç½®
åŒ…å«éšæœºç§å­è®¾ç½®å‡½æ•°ï¼Œç¡®ä¿ç»“æœå¯å¤ç°

```python
def set_seed(seed: int = 42) -> None:
    """
    Set random seed for reproducibility.
    
    Reference: Paper Section X.X (if specified), otherwise using standard practice.
    Note: Full determinism may impact performance on CUDA.
    """
    import random
    import numpy as np
    
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    
    # For deterministic behavior (may reduce performance)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
```

##### 7. éªŒè¯ä»£ç  (Validation)
åœ¨ä»£ç å—æœ«å°¾ï¼ŒåŒ…å«å®Œæ•´çš„éªŒè¯ä»£ç ï¼Œä½¿ç”¨éšæœº Tensor éªŒè¯æ¨¡å‹å¯è¿è¡Œ

```python
if __name__ == "__main__":
    # ========== Configuration ==========
    batch_size = 2
    in_channels = 3
    height, width = 224, 224
    num_classes = 1000
    paper_reported_params = 25_500_000  # From Table X in paper (set None if not reported)

    # ========== Set Seed ==========
    set_seed(42)

    # ========== Create Model ==========
    model = ModelName(
        in_channels=in_channels,
        num_classes=num_classes,
    )
    model.eval()

    # ========== Generate Random Input ==========
    x = torch.randn(batch_size, in_channels, height, width)

    # ========== Forward Pass ==========
    with torch.no_grad():
        output = model(x)

    # ========== Shape Validation ==========
    expected_shape = (batch_size, num_classes)
    assert output.shape == expected_shape, \
        f"âŒ Shape mismatch! Expected {expected_shape}, got {output.shape}"
    print(f"âœ… Output shape: {output.shape}")

    # ========== Parameter Count Validation ==========
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    print(f"ğŸ“Š Total parameters: {total_params:,}")
    print(f"ğŸ“Š Trainable parameters: {trainable_params:,}")

    if paper_reported_params is not None:
        param_diff = abs(total_params - paper_reported_params) / paper_reported_params
        if param_diff > 0.01:
            print(f"âš ï¸  Parameter count differs from paper by {param_diff:.2%}")
        else:
            print(f"âœ… Parameter count matches paper (within 1%)")

    # ========== Gradient Flow Check ==========
    model.train()
    x_grad = torch.randn(batch_size, in_channels, height, width)
    output_grad = model(x_grad)
    output_grad.sum().backward()
    
    no_grad_params = []
    for name, param in model.named_parameters():
        if param.grad is None:
            no_grad_params.append(name)
    
    if no_grad_params:
        print(f"âš ï¸  No gradient for: {no_grad_params}")
    else:
        print("âœ… Gradient flow check passed")

    # ========== Output Range Check ==========
    model.eval()
    with torch.no_grad():
        output_check = model(x)
    print(f"ğŸ“Š Output range: [{output_check.min():.4f}, {output_check.max():.4f}]")

    # ========== Memory Estimation ==========
    param_memory_mb = sum(p.numel() * p.element_size() for p in model.parameters()) / (1024**2)
    print(f"ğŸ“Š Model parameter memory: {param_memory_mb:.2f} MB")

    print("\nâœ… All validations passed! Model is ready for training.")
```

---

### Phase 4: æ¨¡å—æ–‡æ¡£ç”Ÿæˆ (Module Documentation)

åœ¨å®Œæˆä»£ç å®ç°åï¼Œå¿…é¡»ä¸ºæ¯ä¸ªæ ¸å¿ƒæ¨¡å—ç”Ÿæˆè¯¦ç»†çš„æŠ€æœ¯æ–‡æ¡£ã€‚

#### 4.1 æ–‡æ¡£ç»“æ„è¦æ±‚

æ¯ä¸ªæ¨¡å—æ–‡æ¡£å¿…é¡»åŒ…å«ä»¥ä¸‹éƒ¨åˆ†ï¼š

```markdown
# [æ¨¡å—åç§°] æŠ€æœ¯æ–‡æ¡£

## 1. æ¨¡å—æ¦‚è¿°
- åŠŸèƒ½æè¿°
- åœ¨æ•´ä½“æ¶æ„ä¸­çš„ä½ç½®
- è¾“å…¥è¾“å‡ºè§„æ ¼

## 2. æ•°å­¦åŸç†
- æ ¸å¿ƒå…¬å¼ (LaTeX)
- ç¬¦å·è¯´æ˜è¡¨
- å…¬å¼æ¨å¯¼è¿‡ç¨‹ (å¦‚é€‚ç”¨)

## 3. æ•°æ®æµå›¾
- Mermaid æµç¨‹å›¾
- å¼ é‡å½¢çŠ¶å˜åŒ–è¿½è¸ª

## 4. å®ç°ç»†èŠ‚
- å…³é”®ä»£ç ç‰‡æ®µ
- è¶…å‚æ•°è¯´æ˜
- è®¾è®¡å†³ç­–è¯´æ˜

## 5. ä½¿ç”¨ç¤ºä¾‹
- ä»£ç ç¤ºä¾‹
- è¾“å…¥è¾“å‡ºç¤ºä¾‹

## 6. æ³¨æ„äº‹é¡¹
- å¸¸è§é—®é¢˜
- æ€§èƒ½è€ƒé‡
- ä¸è®ºæ–‡çš„å·®å¼‚ (å¦‚æœ‰)
```

#### 4.2 Mermaid æµç¨‹å›¾è§„èŒƒ

##### 4.2.1 æ¨¡å—å†…éƒ¨æµç¨‹å›¾

å±•ç¤ºæ¨¡å—å†…éƒ¨çš„è¯¦ç»†æ•°æ®æµï¼š

```markdown
## æ•°æ®æµå›¾

### æ•´ä½“æµç¨‹

```mermaid
flowchart TB
    subgraph Input["è¾“å…¥å±‚"]
        I[è¾“å…¥å¼ é‡<br/>B, C_in, H, W]
    end

    subgraph Process["å¤„ç†æµç¨‹"]
        direction TB
        P1[æ­¥éª¤1: ç‰¹å¾æå–<br/>B, C_mid, H, W]
        P2[æ­¥éª¤2: å˜æ¢æ“ä½œ<br/>B, C_mid, H', W']
        P3[æ­¥éª¤3: ç‰¹å¾èåˆ<br/>B, C_out, H', W']
    end

    subgraph Output["è¾“å‡ºå±‚"]
        O[è¾“å‡ºå¼ é‡<br/>B, C_out, H', W']
    end

    I --> P1 --> P2 --> P3 --> O

    style I fill:#e1f5fe,stroke:#01579b
    style P1 fill:#fff3e0,stroke:#e65100
    style P2 fill:#fff3e0,stroke:#e65100
    style P3 fill:#fff3e0,stroke:#e65100
    style O fill:#e8f5e9,stroke:#2e7d32
```
```

##### 4.2.2 å¼ é‡å½¢çŠ¶å˜åŒ–å›¾

ä¸“é—¨å±•ç¤ºå¼ é‡ç»´åº¦çš„å˜åŒ–è¿‡ç¨‹ï¼š

```markdown
### å¼ é‡å½¢çŠ¶å˜åŒ–

```mermaid
flowchart LR
    subgraph Shapes["ç»´åº¦å˜åŒ–è¿½è¸ª"]
        S1["[B, 3, 224, 224]"]
        S2["[B, 64, 112, 112]"]
        S3["[B, 64, 56, 56]"]
        S4["[B, 256, 56, 56]"]
        S5["[B, 512, 28, 28]"]
    end

    S1 -->|"Conv 7Ã—7, s=2"| S2
    S2 -->|"MaxPool 3Ã—3, s=2"| S3
    S3 -->|"Bottleneck Ã—3"| S4
    S4 -->|"Bottleneck Ã—4, s=2"| S5

    style S1 fill:#e3f2fd
    style S2 fill:#e8f5e9
    style S3 fill:#fff3e0
    style S4 fill:#fce4ec
    style S5 fill:#f3e5f5
```
```

##### 4.2.3 æ³¨æ„åŠ›æœºåˆ¶æµç¨‹å›¾ (å¦‚é€‚ç”¨)

```markdown
### æ³¨æ„åŠ›è®¡ç®—æµç¨‹

```mermaid
flowchart TB
    subgraph Input
        X[è¾“å…¥ X<br/>B, N, D]
    end

    subgraph QKV["Q, K, V æŠ•å½±"]
        Q[Query<br/>B, N, D]
        K[Key<br/>B, N, D]
        V[Value<br/>B, N, D]
    end

    subgraph Attention["æ³¨æ„åŠ›è®¡ç®—"]
        A1["Q Ã— K^T<br/>B, N, N"]
        A2["Softmax<br/>B, N, N"]
        A3["Attn Ã— V<br/>B, N, D"]
    end

    subgraph Output
        O[è¾“å‡º<br/>B, N, D]
    end

    X --> Q & K & V
    Q & K --> A1
    A1 -->|"Ã· âˆšd_k"| A2
    A2 & V --> A3
    A3 --> O

    style X fill:#e1f5fe,stroke:#01579b
    style Q fill:#fff3e0,stroke:#e65100
    style K fill:#fff3e0,stroke:#e65100
    style V fill:#fff3e0,stroke:#e65100
    style A1 fill:#fce4ec,stroke:#c2185b
    style A2 fill:#fce4ec,stroke:#c2185b
    style A3 fill:#fce4ec,stroke:#c2185b
    style O fill:#e8f5e9,stroke:#2e7d32
```
```

##### 4.2.4 æ®‹å·®è¿æ¥æµç¨‹å›¾

```markdown
### æ®‹å·®è¿æ¥ç»“æ„

```mermaid
flowchart TB
    subgraph Block["æ®‹å·®å—"]
        I[è¾“å…¥ x]
        
        subgraph Main["ä¸»è·¯å¾„ F(x)"]
            M1[Layer 1]
            M2[Layer 2]
            M3[Layer 3]
        end
        
        subgraph Skip["è·³è·ƒè¿æ¥"]
            S[Identity / Projection]
        end
        
        Add((+))
        O[è¾“å‡º y = F(x) + x]
    end

    I --> M1 --> M2 --> M3 --> Add
    I --> S --> Add
    Add --> O

    style I fill:#e1f5fe
    style M1 fill:#fff3e0
    style M2 fill:#fff3e0
    style M3 fill:#fff3e0
    style S fill:#fce4ec
    style Add fill:#f3e5f5
    style O fill:#e8f5e9
```
```

#### 4.3 å…¬å¼è¯´æ˜è§„èŒƒ

##### 4.3.1 æ ¸å¿ƒå…¬å¼æ ¼å¼

```markdown
## æ•°å­¦åŸç†

### æ ¸å¿ƒå…¬å¼

**[å…¬å¼åç§°]**:

$$
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
$$

**å…¬å¼è§£é‡Š**:
- è¯¥å…¬å¼è®¡ç®—ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ› (Scaled Dot-Product Attention)
- ç¼©æ”¾å› å­ $\sqrt{d_k}$ é˜²æ­¢ç‚¹ç§¯å€¼è¿‡å¤§å¯¼è‡´ softmax æ¢¯åº¦æ¶ˆå¤±

**ç¬¦å·è¯´æ˜**:

| ç¬¦å· | å«ä¹‰ | ç»´åº¦ | å–å€¼èŒƒå›´ |
|------|------|------|----------|
| $Q$ | Query çŸ©é˜µ | $[B, N, d_k]$ | $\mathbb{R}$ |
| $K$ | Key çŸ©é˜µ | $[B, N, d_k]$ | $\mathbb{R}$ |
| $V$ | Value çŸ©é˜µ | $[B, N, d_v]$ | $\mathbb{R}$ |
| $d_k$ | Key ç»´åº¦ | scalar | é€šå¸¸ 64 |
| $N$ | åºåˆ—é•¿åº¦ | scalar | > 0 |
| $B$ | æ‰¹æ¬¡å¤§å° | scalar | > 0 |

**ç»´åº¦æ¨å¯¼**:

1. $QK^T$: $[B, N, d_k] \times [B, d_k, N] = [B, N, N]$
2. $\text{softmax}(\cdot)$: $[B, N, N]$ (æ²¿æœ€åä¸€ç»´å½’ä¸€åŒ–)
3. $\text{Attn} \times V$: $[B, N, N] \times [B, N, d_v] = [B, N, d_v]$
```

##### 4.3.2 å…¬å¼æ¨å¯¼è¿‡ç¨‹ (å¤æ‚å…¬å¼)

```markdown
### å…¬å¼æ¨å¯¼

**ç›®æ ‡**: æ¨å¯¼ Layer Normalization çš„åå‘ä¼ æ’­å…¬å¼

**å‰å‘ä¼ æ’­**:

$$
\hat{x}_i = \frac{x_i - \mu}{\sqrt{\sigma^2 + \epsilon}}
$$

$$
y_i = \gamma \hat{x}_i + \beta
$$

å…¶ä¸­:
- $\mu = \frac{1}{D}\sum_{i=1}^{D} x_i$ (å‡å€¼)
- $\sigma^2 = \frac{1}{D}\sum_{i=1}^{D} (x_i - \mu)^2$ (æ–¹å·®)

**åå‘ä¼ æ’­æ¨å¯¼**:

Step 1: è®¡ç®— $\frac{\partial \mathcal{L}}{\partial \hat{x}_i}$

$$
\frac{\partial \mathcal{L}}{\partial \hat{x}_i} = \frac{\partial \mathcal{L}}{\partial y_i} \cdot \gamma
$$

Step 2: è®¡ç®— $\frac{\partial \mathcal{L}}{\partial \sigma^2}$

$$
\frac{\partial \mathcal{L}}{\partial \sigma^2} = \sum_{i=1}^{D} \frac{\partial \mathcal{L}}{\partial \hat{x}_i} \cdot (x_i - \mu) \cdot \left(-\frac{1}{2}\right)(\sigma^2 + \epsilon)^{-3/2}
$$

Step 3: è®¡ç®— $\frac{\partial \mathcal{L}}{\partial \mu}$ ... (ç»§ç»­æ¨å¯¼)
```

##### 4.3.3 Loss Function è¯¦è§£

```markdown
### Loss Function åˆ†æ

**æ€»æŸå¤±å‡½æ•°**:

$$
\mathcal{L}_{total} = \mathcal{L}_{cls} + \lambda_1 \mathcal{L}_{reg} + \lambda_2 \mathcal{L}_{aux}
$$

**å„é¡¹è¯´æ˜**:

| æŸå¤±é¡¹ | å…¬å¼ | ä½œç”¨ | æƒé‡ |
|--------|------|------|------|
| $\mathcal{L}_{cls}$ | $-\sum_i y_i \log(\hat{y}_i)$ | åˆ†ç±»æŸå¤± | 1.0 |
| $\mathcal{L}_{reg}$ | $\|\|W\|\|_2^2$ | L2 æ­£åˆ™åŒ– | $\lambda_1 = 0.01$ |
| $\mathcal{L}_{aux}$ | è§ä¸‹æ–‡ | è¾…åŠ©ç›‘ç£ | $\lambda_2 = 0.4$ |

**è¾…åŠ©æŸå¤±è¯¦è§£**:

$$
\mathcal{L}_{aux} = \text{CrossEntropy}(\text{AuxHead}(f_{mid}), y)
$$

- ä½œç”¨: åœ¨ä¸­é—´å±‚æ·»åŠ ç›‘ç£ä¿¡å·ï¼Œç¼“è§£æ¢¯åº¦æ¶ˆå¤±
- æ¥æº: Paper Section 4.2
- æ³¨æ„: ä»…åœ¨è®­ç»ƒæ—¶ä½¿ç”¨ï¼Œæ¨ç†æ—¶ç§»é™¤
```

#### 4.4 æ–‡æ¡£è¾“å‡ºç»“æ„

å®Œæˆå¤ç°åï¼Œåº”ç”Ÿæˆä»¥ä¸‹æ–‡æ¡£ç»“æ„ï¼š

```
output/
â”œâ”€â”€ README.md                    # é¡¹ç›®æ€»è§ˆ
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ architecture.md          # æ•´ä½“æ¶æ„æ–‡æ¡£
â”‚   â”œâ”€â”€ modules/
â”‚   â”‚   â”œâ”€â”€ backbone.md          # éª¨å¹²ç½‘ç»œæ–‡æ¡£
â”‚   â”‚   â”œâ”€â”€ attention.md         # æ³¨æ„åŠ›æ¨¡å—æ–‡æ¡£
â”‚   â”‚   â”œâ”€â”€ head.md              # è¾“å‡ºå¤´æ–‡æ¡£
â”‚   â”‚   â””â”€â”€ loss.md              # æŸå¤±å‡½æ•°æ–‡æ¡£
â”‚   â”œâ”€â”€ math/
â”‚   â”‚   â”œâ”€â”€ formulas.md          # å…¬å¼æ±‡æ€»
â”‚   â”‚   â””â”€â”€ derivations.md       # æ¨å¯¼è¿‡ç¨‹
â”‚   â””â”€â”€ diagrams/
â”‚       â”œâ”€â”€ overview.md          # æ•´ä½“æµç¨‹å›¾
â”‚       â””â”€â”€ tensor_shapes.md     # å¼ é‡å½¢çŠ¶å˜åŒ–å›¾
â”œâ”€â”€ model.py
â”œâ”€â”€ modules/
â”‚   â””â”€â”€ ...
â””â”€â”€ requirements.txt
```

#### 4.5 æ¨¡å—æ–‡æ¡£ç¤ºä¾‹

```markdown
# Bottleneck Block æŠ€æœ¯æ–‡æ¡£

## 1. æ¨¡å—æ¦‚è¿°

**åŠŸèƒ½**: ResNet çš„æ ¸å¿ƒæ„å»ºå—ï¼Œé€šè¿‡æ®‹å·®è¿æ¥å®ç°æ·±å±‚ç½‘ç»œçš„æœ‰æ•ˆè®­ç»ƒã€‚

**æ¶æ„ä½ç½®**: ä½äº ResNet çš„ Stage 2-5ï¼Œæ¯ä¸ª Stage åŒ…å«å¤šä¸ª Bottleneck å—ã€‚

**è¾“å…¥è¾“å‡ºè§„æ ¼**:
- è¾“å…¥: `[B, C_in, H, W]`
- è¾“å‡º: `[B, C_out, H', W']`
- å…¶ä¸­ `C_out = C_mid Ã— 4` (expansion factor)

## 2. æ•°å­¦åŸç†

### æ®‹å·®æ˜ å°„å…¬å¼

$$
\mathbf{y} = \mathcal{F}(\mathbf{x}, \{W_i\}) + \mathcal{G}(\mathbf{x})
$$

å…¶ä¸­:

$$
\mathcal{F}(\mathbf{x}) = W_3 \cdot \text{ReLU}(W_2 \cdot \text{ReLU}(W_1 \cdot \mathbf{x}))
$$

$$
\mathcal{G}(\mathbf{x}) = \begin{cases} 
\mathbf{x} & \text{if } C_{in} = C_{out} \text{ and } s = 1 \\
W_s \cdot \mathbf{x} & \text{otherwise}
\end{cases}
$$

**ç¬¦å·è¯´æ˜**:

| ç¬¦å· | å«ä¹‰ | ç»´åº¦ |
|------|------|------|
| $\mathbf{x}$ | è¾“å…¥ç‰¹å¾ | $[B, C_{in}, H, W]$ |
| $W_1$ | 1Ã—1 å·ç§¯ (é™ç»´) | $[C_{mid}, C_{in}, 1, 1]$ |
| $W_2$ | 3Ã—3 å·ç§¯ (ç©ºé—´) | $[C_{mid}, C_{mid}, 3, 3]$ |
| $W_3$ | 1Ã—1 å·ç§¯ (å‡ç»´) | $[C_{out}, C_{mid}, 1, 1]$ |
| $W_s$ | 1Ã—1 å·ç§¯ (æŠ•å½±) | $[C_{out}, C_{in}, 1, 1]$ |

## 3. æ•°æ®æµå›¾

```mermaid
flowchart TB
    subgraph Input
        X["x: [B, C_in, H, W]"]
    end

    subgraph MainPath["ä¸»è·¯å¾„ F(x)"]
        C1["Conv1Ã—1 + BN + ReLU<br/>[B, C_mid, H, W]"]
        C2["Conv3Ã—3 + BN + ReLU<br/>[B, C_mid, H', W']"]
        C3["Conv1Ã—1 + BN<br/>[B, C_out, H', W']"]
    end

    subgraph SkipPath["è·³è·ƒè¿æ¥ G(x)"]
        S["Identity / Conv1Ã—1+BN<br/>[B, C_out, H', W']"]
    end

    subgraph Output
        Add(("+"))
        R["ReLU"]
        Y["y: [B, C_out, H', W']"]
    end

    X --> C1 --> C2 --> C3 --> Add
    X --> S --> Add
    Add --> R --> Y

    style X fill:#e1f5fe,stroke:#01579b
    style C1 fill:#fff3e0,stroke:#e65100
    style C2 fill:#fff3e0,stroke:#e65100
    style C3 fill:#fff3e0,stroke:#e65100
    style S fill:#fce4ec,stroke:#c2185b
    style Add fill:#f3e5f5,stroke:#7b1fa2
    style Y fill:#e8f5e9,stroke:#2e7d32
```

### å¼ é‡å½¢çŠ¶å˜åŒ– (stride=2 ç¤ºä¾‹)

```mermaid
flowchart LR
    S1["[B, 256, 56, 56]"]
    S2["[B, 128, 56, 56]"]
    S3["[B, 128, 28, 28]"]
    S4["[B, 512, 28, 28]"]

    S1 -->|"Conv1Ã—1"| S2
    S2 -->|"Conv3Ã—3, s=2"| S3
    S3 -->|"Conv1Ã—1"| S4

    style S1 fill:#e3f2fd
    style S2 fill:#fff3e0
    style S3 fill:#fff3e0
    style S4 fill:#e8f5e9
```

## 4. å®ç°ç»†èŠ‚

### å…³é”®ä»£ç 

```python
class Bottleneck(nn.Module):
    expansion = 4  # C_out = C_mid Ã— 4

    def __init__(self, in_ch, mid_ch, stride=1, downsample=None):
        super().__init__()
        out_ch = mid_ch * self.expansion

        # 1Ã—1 conv: reduce channels
        self.conv1 = nn.Conv2d(in_ch, mid_ch, 1, bias=False)
        self.bn1 = nn.BatchNorm2d(mid_ch)

        # 3Ã—3 conv: spatial processing (may downsample)
        self.conv2 = nn.Conv2d(mid_ch, mid_ch, 3, stride, 1, bias=False)
        self.bn2 = nn.BatchNorm2d(mid_ch)

        # 1Ã—1 conv: expand channels
        self.conv3 = nn.Conv2d(mid_ch, out_ch, 1, bias=False)
        self.bn3 = nn.BatchNorm2d(out_ch)

        self.relu = nn.ReLU(inplace=True)
        self.downsample = downsample
```

### è®¾è®¡å†³ç­–

| å†³ç­– | é€‰æ‹© | åŸå›  |
|------|------|------|
| bias=False | å·ç§¯ä¸ä½¿ç”¨åç½® | BN ä¼šå­¦ä¹ åç½®ï¼Œé¿å…å†—ä½™ |
| ReLU inplace | True | èŠ‚çœå†…å­˜ |
| ä¸‹é‡‡æ ·ä½ç½® | 3Ã—3 conv | è®ºæ–‡åŸå§‹è®¾è®¡ |

## 5. ä½¿ç”¨ç¤ºä¾‹

```python
# åˆ›å»º Bottleneck (æ— ä¸‹é‡‡æ ·)
block = Bottleneck(in_ch=256, mid_ch=64)
x = torch.randn(2, 256, 56, 56)
y = block(x)  # [2, 256, 56, 56]

# åˆ›å»º Bottleneck (æœ‰ä¸‹é‡‡æ ·)
downsample = nn.Sequential(
    nn.Conv2d(256, 512, 1, stride=2, bias=False),
    nn.BatchNorm2d(512)
)
block = Bottleneck(in_ch=256, mid_ch=128, stride=2, downsample=downsample)
x = torch.randn(2, 256, 56, 56)
y = block(x)  # [2, 512, 28, 28]
```

## 6. æ³¨æ„äº‹é¡¹

### å¸¸è§é—®é¢˜

1. **ç»´åº¦ä¸åŒ¹é…**: å½“ stride > 1 æˆ–é€šé“æ•°å˜åŒ–æ—¶ï¼Œå¿…é¡»æä¾› downsample
2. **æ¢¯åº¦æ¶ˆå¤±**: æ®‹å·®è¿æ¥ç¡®ä¿æ¢¯åº¦å¯ä»¥ç›´æ¥å›ä¼ 

### æ€§èƒ½è€ƒé‡

- 1Ã—1 å·ç§¯å‡å°‘è®¡ç®—é‡çº¦ 9 å€ (ç›¸æ¯”ç›´æ¥ä½¿ç”¨ 3Ã—3)
- inplace ReLU èŠ‚çœçº¦ 50% æ¿€æ´»å†…å­˜

### ä¸è®ºæ–‡å·®å¼‚

æ— å·®å¼‚ï¼Œå®Œå…¨æŒ‰ç…§è®ºæ–‡ Section 3.3 å®ç°ã€‚
```

---

## "Paper Did Not Specify" Protocol

å½“è®ºæ–‡æœªæ˜ç¡®è¯´æ˜æŸä¸ªè¶…å‚æ•°æˆ–å®ç°ç»†èŠ‚æ—¶ï¼Œå¿…é¡»éµå¾ªä»¥ä¸‹è§„èŒƒï¼š

### æ ‡æ³¨æ ¼å¼

```python
# [Parameter Name]: Paper did not specify
# Recommended: [value]
# Reference: [å…·ä½“æ¥æºï¼Œå¦‚ "BERT (Devlin et al., 2019)" æˆ– "Common practice in ViT variants"]
# Alternatives: [å…¶ä»–å¯é€‰å€¼ï¼Œå¦‚ "[0.0, 0.1, 0.2]"]
# Impact: [ç®€è¿°è¯¥å‚æ•°å¯¹æ€§èƒ½çš„æ½œåœ¨å½±å“ï¼Œå¦‚ "Higher values may cause underfitting"]
self.dropout = nn.Dropout(p=0.1)
```

### ç¤ºä¾‹

```python
# Dropout rate: Paper did not specify
# Recommended: 0.1
# Reference: BERT (Devlin et al., 2019), ViT (Dosovitskiy et al., 2021)
# Alternatives: [0.0, 0.1, 0.2, 0.3]
# Impact: Higher dropout may improve generalization but slow convergence
self.dropout = nn.Dropout(p=0.1)

# Weight decay: Paper did not specify
# Recommended: 0.01
# Reference: AdamW default in Transformer architectures
# Alternatives: [0.0, 0.01, 0.05, 0.1]
# Impact: Higher values provide stronger regularization
```

---

## Common Pitfalls Warning âš ï¸

åœ¨å¤ç°è¿‡ç¨‹ä¸­ï¼Œæ³¨æ„ä»¥ä¸‹å¸¸è§é™·é˜±ï¼š

### 1. Tensor æ“ä½œé™·é˜±
```python
# âŒ é”™è¯¯: reshape å¯èƒ½ç ´åå†…å­˜è¿ç»­æ€§
x = x.reshape(B, -1)

# âœ… æ­£ç¡®: ä½¿ç”¨ view (è¦æ±‚è¿ç»­) æˆ– flatten
x = x.view(B, -1)  # å¦‚æœå·²è¿ç»­
x = x.flatten(start_dim=1)  # æ›´å®‰å…¨
x = x.contiguous().view(B, -1)  # ç¡®ä¿è¿ç»­åå† view
```

### 2. è®ºæ–‡å›¾è¡¨å¯èƒ½æœ‰è¯¯
- ä½œè€…å¯èƒ½ç”»é”™äº†æ¶æ„å›¾ï¼Œ**ä»¥æ–‡å­—æè¿°å’Œå…¬å¼ä¸ºå‡†**
- å¦‚å‘ç°å›¾æ–‡ä¸ä¸€è‡´ï¼Œåœ¨æ³¨é‡Šä¸­æ ‡æ³¨

### 3. è¶…å‚æ•°é™·é˜±
- åŒºåˆ†"è®­ç»ƒæ—¶ä½¿ç”¨çš„è¶…å‚æ•°" vs "æœ€ç»ˆæŠ¥å‘Šçš„æœ€ä½³è¶…å‚æ•°"
- è®ºæ–‡å¯èƒ½æŠ¥å‘Šäº†å¤šæ¬¡å®éªŒçš„æœ€ä¼˜å€¼ï¼Œè€Œéå•æ¬¡è®­ç»ƒé…ç½®

### 4. å®˜æ–¹ä»£ç ä¸è®ºæ–‡ä¸ä¸€è‡´
- å¦‚æœè®ºæ–‡æä¾›äº†å®˜æ–¹ä»£ç ï¼Œ**ä¼˜å…ˆå‚è€ƒä»£ç å®ç°**
- åœ¨æ³¨é‡Šä¸­æ ‡æ³¨ä»£ç ä¸è®ºæ–‡æè¿°çš„å·®å¼‚

### 5. æ¡†æ¶å·®å¼‚
```python
# âš ï¸ BatchNorm momentum åœ¨ PyTorch å’Œ TensorFlow ä¸­å®šä¹‰ä¸åŒ!
# PyTorch: new_running_mean = (1 - momentum) * running_mean + momentum * batch_mean
# TensorFlow: new_running_mean = momentum * running_mean + (1 - momentum) * batch_mean
# 
# å¦‚æœè®ºæ–‡ä½¿ç”¨ TensorFlow ä¸” momentum=0.9ï¼Œåœ¨ PyTorch ä¸­åº”è®¾ä¸º 0.1
nn.BatchNorm2d(channels, momentum=0.1)  # Equivalent to TF momentum=0.9
```

### 6. åˆå§‹åŒ–å·®å¼‚
- ä¸åŒæ¡†æ¶çš„é»˜è®¤åˆå§‹åŒ–æ–¹å¼ä¸åŒ
- å¦‚è®ºæ–‡æœªæŒ‡å®šï¼Œä¼˜å…ˆä½¿ç”¨ PyTorch é»˜è®¤å€¼å¹¶æ ‡æ³¨

---

## PDF Reading Guidelines

å½“è¯»å– PDF è®ºæ–‡æ—¶ï¼Œéœ€è¦ç‰¹åˆ«å…³æ³¨ï¼š

### 1. å›¾è¡¨è§£æ
- **Figure**: ç½‘ç»œæ¶æ„å›¾ã€æ•°æ®æµå›¾ã€å®éªŒç»“æœå¯è§†åŒ–
- **Table**: è¶…å‚æ•°é…ç½®ã€å®éªŒå¯¹æ¯”ã€æ¶ˆèå®éªŒç»“æœ
- **Algorithm**: ä¼ªä»£ç ã€è®­ç»ƒæµç¨‹

### 2. å…¬å¼æå–
- ä¸»è¦å…³æ³¨ Method/Approach ç« èŠ‚çš„å…¬å¼
- æ³¨æ„å…¬å¼ç¼–å·ï¼Œä¾¿äºå¼•ç”¨
- æ£€æŸ¥ Appendix ä¸­çš„è¡¥å……å…¬å¼

### 3. å®ç°ç»†èŠ‚
- **Section 3/4 (Method)**: æ ¸å¿ƒæ¶æ„æè¿°
- **Section 5 (Experiments)**: è®­ç»ƒç»†èŠ‚ã€è¶…å‚æ•°
- **Appendix**: é¢å¤–å®ç°ç»†èŠ‚ã€å®Œæ•´é…ç½®

### 4. ä»£ç å¼•ç”¨
å¦‚æœè®ºæ–‡æä¾›äº†å®˜æ–¹ä»£ç ä»“åº“ï¼š
- ä¼˜å…ˆå‚è€ƒå®˜æ–¹å®ç°
- æ ‡æ³¨ä»£ç ç‰ˆæœ¬/commit hash
- å¯¹æ¯”è®ºæ–‡æè¿°ä¸ä»£ç å®ç°çš„å·®å¼‚

---

## Output Structure

æœ€ç»ˆè¾“å‡ºåº”åŒ…å«ä»¥ä¸‹æ–‡ä»¶ç»“æ„ï¼š

```
output/
â”œâ”€â”€ README.md                    # è®ºæ–‡æ¦‚è¿°ã€ä½¿ç”¨è¯´æ˜
â”œâ”€â”€ docs/                        # æŠ€æœ¯æ–‡æ¡£
â”‚   â”œâ”€â”€ architecture.md          # æ•´ä½“æ¶æ„æ–‡æ¡£
â”‚   â”œâ”€â”€ modules/                 # æ¨¡å—æ–‡æ¡£
â”‚   â”‚   â”œâ”€â”€ backbone.md          # éª¨å¹²ç½‘ç»œæ–‡æ¡£
â”‚   â”‚   â”œâ”€â”€ attention.md         # æ³¨æ„åŠ›æ¨¡å—æ–‡æ¡£ (å¦‚é€‚ç”¨)
â”‚   â”‚   â”œâ”€â”€ head.md              # è¾“å‡ºå¤´æ–‡æ¡£
â”‚   â”‚   â””â”€â”€ loss.md              # æŸå¤±å‡½æ•°æ–‡æ¡£
â”‚   â”œâ”€â”€ math/                    # æ•°å­¦æ–‡æ¡£
â”‚   â”‚   â”œâ”€â”€ formulas.md          # å…¬å¼æ±‡æ€»
â”‚   â”‚   â””â”€â”€ derivations.md       # æ¨å¯¼è¿‡ç¨‹ (å¦‚é€‚ç”¨)
â”‚   â””â”€â”€ diagrams/                # æµç¨‹å›¾æ–‡æ¡£
â”‚       â”œâ”€â”€ overview.md          # æ•´ä½“æ¶æ„æµç¨‹å›¾
â”‚       â””â”€â”€ tensor_shapes.md     # å¼ é‡å½¢çŠ¶å˜åŒ–å›¾
â”œâ”€â”€ model.py                     # ä¸»æ¨¡å‹å®ç°
â”œâ”€â”€ modules/                     # å­æ¨¡å—
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ attention.py             # æ³¨æ„åŠ›æ¨¡å—
â”‚   â”œâ”€â”€ backbone.py              # éª¨å¹²ç½‘ç»œ
â”‚   â””â”€â”€ head.py                  # è¾“å‡ºå¤´
â”œâ”€â”€ config.py                    # é…ç½®æ–‡ä»¶
â””â”€â”€ requirements.txt             # ä¾èµ–åˆ—è¡¨
```

---

## Constraints

### å·¥å…·æ ˆ
- Python 3.10+
- PyTorch 2.0+
- Mermaid (for diagrams)
- einops (optional, for tensor operations)
- timm (optional, for pretrained backbones)

### è¯­è¨€è§„èŒƒ
- **åˆ†æä¸è§£é‡Š**: ä½¿ç”¨ç®€ä½“ä¸­æ–‡
- **ä»£ç æ³¨é‡Š**: ä½¿ç”¨è‹±æ–‡
- **å˜é‡å‘½å**: ä½¿ç”¨è‹±æ–‡ï¼Œéµå¾ª PEP 8

---

## Verification Checklist

åœ¨å®Œæˆå¤ç°åï¼Œæ£€æŸ¥ä»¥ä¸‹é¡¹ç›®ï¼š

**Phase 1 - è®ºæ–‡è§£æ„**:
- [ ] æ ¸å¿ƒé—®é¢˜å’Œåˆ›æ–°ç‚¹å·²æ˜ç¡®
- [ ] æ‰€æœ‰å…³é”®å…¬å¼å·²æå–å¹¶è§£é‡Š
- [ ] è¶…å‚æ•°é…ç½®å®Œæ•´ï¼ŒæœªæŒ‡å®šé¡¹å·²æŒ‰è§„èŒƒæ ‡æ³¨

**Phase 2 - æ¶æ„å¯è§†åŒ–**:
- [ ] Mermaid æµç¨‹å›¾åŒ…å«æ‰€æœ‰ä¸»è¦ç»„ä»¶
- [ ] æ¯ä¸ªèŠ‚ç‚¹æ ‡æ³¨äº† Tensor Shape (ä½¿ç”¨æ ‡å‡†ç¬¦å·)
- [ ] ä½¿ç”¨é¢œè‰²åŒºåˆ†ä¸åŒç±»å‹ç»„ä»¶
- [ ] æ·±å±‚ç½‘ç»œä½¿ç”¨æ¨¡å—çº§ç²’åº¦

**Phase 3 - ä»£ç å®ç°**:
- [ ] ç»§æ‰¿ `nn.Module`
- [ ] å®Œæ•´çš„ç±»å‹æç¤º
- [ ] æ¯å±‚æ“ä½œåæœ‰ç»´åº¦æ³¨é‡Š (ä½¿ç”¨æ ‡å‡†ç¬¦å·)
- [ ] å­æ¨¡å—å·²æ‹†åˆ†
- [ ] åŒ…å« `_init_weights` æ–¹æ³•
- [ ] åŒ…å« `set_seed` å‡½æ•°
- [ ] æœªæŒ‡å®šå‚æ•°å·²æŒ‰ Protocol æ ‡æ³¨

**Phase 4 - æ¨¡å—æ–‡æ¡£**:
- [ ] æ¯ä¸ªæ ¸å¿ƒæ¨¡å—æœ‰ç‹¬ç«‹æ–‡æ¡£
- [ ] æ–‡æ¡£åŒ…å«å®Œæ•´çš„ 6 ä¸ªéƒ¨åˆ† (æ¦‚è¿°/æ•°å­¦/æµç¨‹å›¾/å®ç°/ç¤ºä¾‹/æ³¨æ„äº‹é¡¹)
- [ ] Mermaid æµç¨‹å›¾æ¸…æ™°å±•ç¤ºæ•°æ®æµ
- [ ] å¼ é‡å½¢çŠ¶å˜åŒ–å›¾å®Œæ•´
- [ ] å…¬å¼ä½¿ç”¨ LaTeX æ ¼å¼å¹¶æœ‰ç¬¦å·è¯´æ˜è¡¨
- [ ] å¤æ‚å…¬å¼åŒ…å«æ¨å¯¼è¿‡ç¨‹
- [ ] Loss Function æœ‰è¯¦ç»†åˆ†æ
- [ ] æ–‡æ¡£ç»“æ„ç¬¦åˆè§„èŒƒ (docs/modules/, docs/math/, docs/diagrams/)

**éªŒè¯æµ‹è¯•**:
- [ ] éšæœº Tensor å‰å‘ä¼ æ’­æ— æŠ¥é”™
- [ ] è¾“å‡º Shape ä¸é¢„æœŸä¸€è‡´
- [ ] å‚æ•°é‡ä¸è®ºæ–‡ä¸€è‡´ (å¦‚æœ‰æŠ¥å‘Šï¼Œå…è®¸ 1% è¯¯å·®)
- [ ] æ¢¯åº¦æµæ£€æŸ¥é€šè¿‡
- [ ] è¾“å‡ºæ•°å€¼èŒƒå›´åˆç†

**å¸¸è§é™·é˜±æ£€æŸ¥**:
- [ ] å·²æ£€æŸ¥ reshape/view çš„ä½¿ç”¨
- [ ] å·²æ ¸å¯¹è®ºæ–‡å›¾è¡¨ä¸æ–‡å­—æè¿°
- [ ] å·²æ³¨æ„æ¡†æ¶å·®å¼‚ (å¦‚ BatchNorm momentum)
