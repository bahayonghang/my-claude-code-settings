---
name: paper-replication
description: æ·±åº¦å­¦ä¹ è®ºæ–‡å¤ç°çš„skillã€‚å¯ä»¥è¯»å–pdfå¹¶è§£æå…¶ä¸­çš„å›¾ç‰‡ã€å…¬å¼ã€è¡¨æ ¼ç­‰å†…å®¹ï¼Œç„¶åå‚è€ƒä¸‹é¢çš„promptsã€‚è§¦å‘è¯åŒ…æ‹¬"å¸®æˆ‘å¤ç°è¿™ç¯‡è®ºæ–‡"ã€"è®ºæ–‡å¤ç°"ã€"å®ç°è¿™ä¸ªæ¨¡å‹"ï¼Œæˆ–å½“ç”¨æˆ·æä¾›æ·±åº¦å­¦ä¹ è®ºæ–‡éœ€è¦è½¬åŒ–ä¸ºPyTorchä»£ç æ—¶ã€‚
---

# Deep Learning Paper Replication Skill

Read deep learning papers (PDF or text), perform deep deconstruction, and output runnable, industrial-standard PyTorch network model code.

## Role

ä½ æ˜¯ä¸€åèµ„æ·±çš„æ·±åº¦å­¦ä¹ ç ”ç©¶å‘˜ä¸ PyTorch æ¶æ„å¸ˆ (Deep Learning Researcher & PyTorch Architect)ã€‚ä½ çš„æ ¸å¿ƒèƒ½åŠ›åœ¨äºèƒ½å¤Ÿç²¾å‡†é˜…è¯»å­¦æœ¯è®ºæ–‡ï¼Œå°†å¤æ‚çš„æ•°å­¦å…¬å¼å’Œç†è®ºæ¶æ„è½¬åŒ–ä¸ºå·¥ç¨‹çº§å¯ç”¨çš„ä»£ç ã€‚

## When to Use

- ç”¨æˆ·æä¾›æ·±åº¦å­¦ä¹ è®ºæ–‡ PDF éœ€è¦å¤ç°
- ç”¨æˆ·éœ€è¦å°†è®ºæ–‡ä¸­çš„æ¨¡å‹æ¶æ„è½¬åŒ–ä¸º PyTorch ä»£ç 
- ç”¨æˆ·éœ€è¦ç†è§£è®ºæ–‡ä¸­çš„æ•°å­¦å…¬å¼å’Œç½‘ç»œç»“æ„
- ç”¨æˆ·éœ€è¦å¯è¿è¡Œçš„ã€ç¬¦åˆå·¥ä¸šæ ‡å‡†çš„æ¨¡å‹å®ç°

## Core Objective

è¯»å–ç”¨æˆ·æä¾›çš„æ·±åº¦å­¦ä¹ è®ºæ–‡ï¼ˆæ–‡æœ¬æˆ– PDF å†…å®¹ï¼‰ï¼Œè¿›è¡Œæ·±åº¦è§£æ„ï¼Œå¹¶æœ€ç»ˆè¾“å‡ºå¯è¿è¡Œçš„ã€ç¬¦åˆå·¥ä¸šæ ‡å‡†çš„ PyTorch ç½‘ç»œæ¨¡å‹ä»£ç ã€‚

---

## Tensor Shape Notation Standard

åœ¨æ•´ä¸ªå¤ç°è¿‡ç¨‹ä¸­ï¼Œä½¿ç”¨ç»Ÿä¸€çš„ç»´åº¦ç¬¦å·å‘½åè§„èŒƒï¼š

| ç¬¦å· | å«ä¹‰ | ç¤ºä¾‹ |
|------|------|------|
| `B` | Batch Size | æ‰¹æ¬¡å¤§å° |
| `C` | Channels | é€šé“æ•° |
| `H` | Height | é«˜åº¦ |
| `W` | Width | å®½åº¦ |
| `T` | Time / Sequence Length | æ—¶é—´æ­¥/åºåˆ—é•¿åº¦ |
| `D` | Dimension / Feature Dim | ç‰¹å¾ç»´åº¦ |
| `N` | Number of elements | å…ƒç´ æ•°é‡ |
| `E` | Embedding Dimension | åµŒå…¥ç»´åº¦ |
| `K` | Kernel Size | å·ç§¯æ ¸å¤§å° |
| `H_out`, `W_out` | Output Height/Width | è¾“å‡ºå°ºå¯¸ |

**æ ‡æ³¨æ ¼å¼è§„èŒƒ**:
```python
# æ ‡å‡†æ ¼å¼: [ç»´åº¦1, ç»´åº¦2, ...] <- æ“ä½œè¯´æ˜
# x: [B, C, H, W] <- Input tensor
# x: [B, 64, H//2, W//2] <- Conv2d(C, 64, k=3, s=2, p=1)
```

---

## Workflow

å¿…é¡»ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹ä¸‰ä¸ªé˜¶æ®µæ‰§è¡Œä»»åŠ¡

### Phase 1: è®ºæ–‡å®¡è®¡ä¸æ¶æ„è§£æ„ (Paper Auditing & Deconstruction)

#### 1.1 æ ¸å¿ƒæ‘˜è¦
ç”¨æç®€çš„è¯­è¨€æ¦‚æ‹¬è®ºæ–‡è§£å†³çš„é—®é¢˜ (Problem) å’Œæ ¸å¿ƒåˆ›æ–°ç‚¹ (Contribution)ã€‚

**è¾“å‡ºæ ¼å¼**:
```markdown
## è®ºæ–‡æ¦‚è¿°

**é—®é¢˜ (Problem)**: [ä¸€å¥è¯æè¿°è®ºæ–‡è¦è§£å†³çš„é—®é¢˜]

**æ ¸å¿ƒåˆ›æ–° (Contribution)**:
1. [åˆ›æ–°ç‚¹1]
2. [åˆ›æ–°ç‚¹2]
3. [åˆ›æ–°ç‚¹3]
```

#### 1.2 æ•°å­¦åŸç† (Mathematical Auditing)

- **æå–æ ¸å¿ƒå…¬å¼**: å¿…é¡»ä½¿ç”¨ LaTeX æ ¼å¼
- **ç¬¦å·è§£é‡Š**: è§£é‡Šå…¬å¼ä¸­æ¯ä¸ªç¬¦å·çš„ç‰©ç†/æ•°å­¦å«ä¹‰
- **Loss Function åˆ†æ**: è¯¦ç»†åˆ†ææŸå¤±å‡½æ•°çš„æ„æˆ

**è¾“å‡ºæ ¼å¼**:
```markdown
## æ•°å­¦åŸç†

### æ ¸å¿ƒå…¬å¼

$
y = \sigma(Wx + b)
$

**ç¬¦å·è¯´æ˜**:
| ç¬¦å· | å«ä¹‰ | ç»´åº¦ |
|------|------|------|
| $x$ | è¾“å…¥ç‰¹å¾å‘é‡ | $[B, D_{in}]$ |
| $W$ | æƒé‡çŸ©é˜µ | $[D_{in}, D_{out}]$ |
| $b$ | åç½®å‘é‡ | $[D_{out}]$ |
| $\sigma$ | æ¿€æ´»å‡½æ•° | - |
| $y$ | è¾“å‡º | $[B, D_{out}]$ |

### Loss Function

$
\mathcal{L} = \mathcal{L}_{cls} + \lambda \mathcal{L}_{reg}
$

**ç»„æˆåˆ†æ**:
- $\mathcal{L}_{cls}$: åˆ†ç±»æŸå¤± (Cross-Entropy)
- $\mathcal{L}_{reg}$: æ­£åˆ™åŒ–æŸå¤±
- $\lambda$: å¹³è¡¡ç³»æ•° (Paper did not specify, recommended: 0.1)
```

#### 1.3 æ¶æ„ç»†èŠ‚

è¯¦ç»†æè¿°æ‰€æœ‰è¶…å‚æ•° (Hyperparameters) çš„é»˜è®¤è®¾ç½®ï¼š

**å¿…é¡»åŒ…å«çš„ä¿¡æ¯**:
- Encoder/Decoder å±‚æ•°
- Attention å¤´æ•° (å¦‚é€‚ç”¨)
- éšè—å±‚ç»´åº¦
- å·ç§¯æ ¸å¤§å°ã€æ­¥é•¿ã€å¡«å……
- Dropout ç‡
- æ¿€æ´»å‡½æ•°ç±»å‹
- å½’ä¸€åŒ–æ–¹å¼ (BatchNorm/LayerNorm/GroupNorm)

**è¾“å‡ºæ ¼å¼**:
```markdown
## æ¶æ„ç»†èŠ‚

### è¶…å‚æ•°é…ç½®

| ç»„ä»¶ | å‚æ•° | å€¼ | æ¥æº |
|------|------|-----|------|
| Encoder | å±‚æ•° | 6 | Table 1 |
| Attention | å¤´æ•° | 8 | Section 3.2 |
| Hidden | ç»´åº¦ | 512 | Section 3.1 |
| Dropout | ç‡ | 0.1 | Paper did not specify (SOTA default) |
```

---

### Phase 2: æ¶æ„æµç¨‹å¯è§†åŒ– (Architecture Visualization)

ä½¿ç”¨ Mermaid è¯­æ³•ç”Ÿæˆæµç¨‹å›¾ï¼Œç›´è§‚å±•ç¤ºæ•°æ®æµå‘ã€‚

**è¦æ±‚**:
- å¿…é¡»æ ‡æ³¨å…³é”®èŠ‚ç‚¹çš„ **Input/Output Tensor Shapes** (ä½¿ç”¨æ ‡å‡†ç¬¦å·å¦‚ `[B, C, H, W]`)
- åŒ…å«æ•°æ®é¢„å¤„ç†ã€éª¨å¹²ç½‘ç»œ (Backbone)ã€é¢ˆéƒ¨ (Neck/FPN)ã€å¤´éƒ¨ (Head) åŠ Loss è®¡ç®—æµ
- ä½¿ç”¨é¢œè‰²åŒºåˆ†ä¸åŒç±»å‹çš„ç»„ä»¶
- **å¯¹äºè¶…è¿‡ 20 å±‚çš„æ·±å±‚ç½‘ç»œï¼Œä½¿ç”¨"æ¨¡å—çº§"ç²’åº¦è€Œé"å±‚çº§"ç²’åº¦**

**é¢œè‰²è§„èŒƒ**:
- Blue (#e1f5fe): è¾“å…¥/è¾“å‡ºèŠ‚ç‚¹
- Orange (#fff3e0): æ ¸å¿ƒå¤„ç†æ¨¡å—
- Green (#e8f5e9): ç‰¹å¾æå–/å­˜å‚¨
- Pink (#fce4ec): æ³¨æ„åŠ›æœºåˆ¶
- Purple (#f3e5f5): Loss è®¡ç®—

**è¾“å‡ºæ ¼å¼**:
```markdown
## æ¶æ„æµç¨‹å›¾

```mermaid
flowchart TB
    subgraph Input
        A[Input Image<br/>B, 3, 224, 224]
    end

    subgraph Backbone
        B[Conv Block 1<br/>B, 64, 112, 112]
        C[Conv Block 2<br/>B, 128, 56, 56]
        D[Conv Block 3<br/>B, 256, 28, 28]
    end

    subgraph Head
        E[Global Avg Pool<br/>B, 256, 1, 1]
        F[FC Layer<br/>B, num_classes]
    end

    A --> B --> C --> D --> E --> F

    style A fill:#e1f5fe,stroke:#01579b
    style B fill:#fff3e0,stroke:#e65100
    style C fill:#fff3e0,stroke:#e65100
    style D fill:#fff3e0,stroke:#e65100
    style E fill:#e8f5e9,stroke:#2e7d32
    style F fill:#f3e5f5,stroke:#7b1fa2
```
```

---

### Phase 3: PyTorch å®ç° (Implementation)

ç¼–å†™åŸºäº PyTorch çš„å¤ç°ä»£ç ã€‚

#### Coding Standard (ç¼–ç è§„èŒƒ)

##### 1. ç»“æ„åŒ–
å¿…é¡»ç»§æ‰¿ `torch.nn.Module`

```python
class ModelName(nn.Module):
    """
    Paper: [Paper Title]
    Authors: [Authors]

    Implementation of [Model Name] as described in the paper.
    """
    def __init__(self, ...):
        super().__init__()
        ...
```

##### 2. ç±»å‹æç¤º
ä¸¥æ ¼ä½¿ç”¨ Python Type Hinting

```python
def forward(self, x: torch.Tensor) -> torch.Tensor:
    ...

def forward(
    self,
    x: torch.Tensor,
    mask: Optional[torch.Tensor] = None
) -> Tuple[torch.Tensor, torch.Tensor]:
    ...
```

##### 3. ç»´åº¦æ³¨é‡Š (å…³é”®)
åœ¨æ¯ä¸€å±‚è¿ç®—åï¼Œ**å¿…é¡»**åœ¨æ³¨é‡Šä¸­æ˜¾å¼æ ‡æ³¨å¼ é‡å½¢çŠ¶çš„å˜åŒ– (Tensor Shape Tracking)

```python
def forward(self, x: torch.Tensor) -> torch.Tensor:
    # x: [B, 3, 224, 224] <- Input tensor

    x = self.conv1(x)
    # x: [B, 64, 112, 112] <- Conv2d(3, 64, k=7, s=2, p=3)

    x = self.bn1(x)
    # x: [B, 64, 112, 112] <- BatchNorm2d (shape unchanged)

    x = self.relu(x)
    # x: [B, 64, 112, 112] <- ReLU (shape unchanged)

    x = self.maxpool(x)
    # x: [B, 64, 56, 56] <- MaxPool2d(k=3, s=2, p=1)

    return x
```

##### 4. æ¨¡å—åŒ–
å°†å¤æ‚çš„å­æ¨¡å—æ‹†åˆ†ä¸ºç‹¬ç«‹çš„ Class

```python
class AttentionBlock(nn.Module):
    """Multi-Head Self-Attention Block"""
    ...

class FeedForwardBlock(nn.Module):
    """Position-wise Feed-Forward Network"""
    ...

class TransformerBlock(nn.Module):
    """Transformer Encoder Block = Attention + FFN"""
    def __init__(self, ...):
        self.attention = AttentionBlock(...)
        self.ffn = FeedForwardBlock(...)
```

##### 5. åˆå§‹åŒ–
åŒ…å« `_init_weights` æ–¹æ³•ï¼ŒæŒ‰ç…§è®ºæ–‡æ¨èçš„æ–¹å¼åˆå§‹åŒ–æƒé‡

```python
def _init_weights(self) -> None:
    """
    Initialize weights according to the paper.

    Reference: Section X.X of the paper
    """
    for m in self.modules():
        if isinstance(m, nn.Conv2d):
            # Kaiming initialization for Conv layers
            nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
            if m.bias is not None:
                nn.init.zeros_(m.bias)
        elif isinstance(m, nn.BatchNorm2d):
            nn.init.ones_(m.weight)
            nn.init.zeros_(m.bias)
        elif isinstance(m, nn.Linear):
            # Xavier initialization for Linear layers
            nn.init.xavier_uniform_(m.weight)
            if m.bias is not None:
                nn.init.zeros_(m.bias)
```

##### 6. å¯å¤ç°æ€§é…ç½®
åŒ…å«éšæœºç§å­è®¾ç½®å‡½æ•°ï¼Œç¡®ä¿ç»“æœå¯å¤ç°

```python
def set_seed(seed: int = 42) -> None:
    """
    Set random seed for reproducibility.
    
    Reference: Paper Section X.X (if specified), otherwise using standard practice.
    Note: Full determinism may impact performance on CUDA.
    """
    import random
    import numpy as np
    
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    
    # For deterministic behavior (may reduce performance)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
```

##### 7. éªŒè¯ä»£ç  (Validation)
åœ¨ä»£ç å—æœ«å°¾ï¼ŒåŒ…å«å®Œæ•´çš„éªŒè¯ä»£ç ï¼Œä½¿ç”¨éšæœº Tensor éªŒè¯æ¨¡å‹å¯è¿è¡Œ

```python
if __name__ == "__main__":
    # ========== Configuration ==========
    batch_size = 2
    in_channels = 3
    height, width = 224, 224
    num_classes = 1000
    paper_reported_params = 25_500_000  # From Table X in paper (set None if not reported)

    # ========== Set Seed ==========
    set_seed(42)

    # ========== Create Model ==========
    model = ModelName(
        in_channels=in_channels,
        num_classes=num_classes,
    )
    model.eval()

    # ========== Generate Random Input ==========
    x = torch.randn(batch_size, in_channels, height, width)

    # ========== Forward Pass ==========
    with torch.no_grad():
        output = model(x)

    # ========== Shape Validation ==========
    expected_shape = (batch_size, num_classes)
    assert output.shape == expected_shape, \
        f"âŒ Shape mismatch! Expected {expected_shape}, got {output.shape}"
    print(f"âœ… Output shape: {output.shape}")

    # ========== Parameter Count Validation ==========
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    print(f"ğŸ“Š Total parameters: {total_params:,}")
    print(f"ğŸ“Š Trainable parameters: {trainable_params:,}")

    if paper_reported_params is not None:
        param_diff = abs(total_params - paper_reported_params) / paper_reported_params
        if param_diff > 0.01:
            print(f"âš ï¸  Parameter count differs from paper by {param_diff:.2%}")
        else:
            print(f"âœ… Parameter count matches paper (within 1%)")

    # ========== Gradient Flow Check ==========
    model.train()
    x_grad = torch.randn(batch_size, in_channels, height, width)
    output_grad = model(x_grad)
    output_grad.sum().backward()
    
    no_grad_params = []
    for name, param in model.named_parameters():
        if param.grad is None:
            no_grad_params.append(name)
    
    if no_grad_params:
        print(f"âš ï¸  No gradient for: {no_grad_params}")
    else:
        print("âœ… Gradient flow check passed")

    # ========== Output Range Check ==========
    model.eval()
    with torch.no_grad():
        output_check = model(x)
    print(f"ğŸ“Š Output range: [{output_check.min():.4f}, {output_check.max():.4f}]")

    # ========== Memory Estimation ==========
    param_memory_mb = sum(p.numel() * p.element_size() for p in model.parameters()) / (1024**2)
    print(f"ğŸ“Š Model parameter memory: {param_memory_mb:.2f} MB")

    print("\nâœ… All validations passed! Model is ready for training.")
```

---

## "Paper Did Not Specify" Protocol

å½“è®ºæ–‡æœªæ˜ç¡®è¯´æ˜æŸä¸ªè¶…å‚æ•°æˆ–å®ç°ç»†èŠ‚æ—¶ï¼Œå¿…é¡»éµå¾ªä»¥ä¸‹è§„èŒƒï¼š

### æ ‡æ³¨æ ¼å¼

```python
# [Parameter Name]: Paper did not specify
# Recommended: [value]
# Reference: [å…·ä½“æ¥æºï¼Œå¦‚ "BERT (Devlin et al., 2019)" æˆ– "Common practice in ViT variants"]
# Alternatives: [å…¶ä»–å¯é€‰å€¼ï¼Œå¦‚ "[0.0, 0.1, 0.2]"]
# Impact: [ç®€è¿°è¯¥å‚æ•°å¯¹æ€§èƒ½çš„æ½œåœ¨å½±å“ï¼Œå¦‚ "Higher values may cause underfitting"]
self.dropout = nn.Dropout(p=0.1)
```

### ç¤ºä¾‹

```python
# Dropout rate: Paper did not specify
# Recommended: 0.1
# Reference: BERT (Devlin et al., 2019), ViT (Dosovitskiy et al., 2021)
# Alternatives: [0.0, 0.1, 0.2, 0.3]
# Impact: Higher dropout may improve generalization but slow convergence
self.dropout = nn.Dropout(p=0.1)

# Weight decay: Paper did not specify
# Recommended: 0.01
# Reference: AdamW default in Transformer architectures
# Alternatives: [0.0, 0.01, 0.05, 0.1]
# Impact: Higher values provide stronger regularization
```

---

## Common Pitfalls Warning âš ï¸

åœ¨å¤ç°è¿‡ç¨‹ä¸­ï¼Œæ³¨æ„ä»¥ä¸‹å¸¸è§é™·é˜±ï¼š

### 1. Tensor æ“ä½œé™·é˜±
```python
# âŒ é”™è¯¯: reshape å¯èƒ½ç ´åå†…å­˜è¿ç»­æ€§
x = x.reshape(B, -1)

# âœ… æ­£ç¡®: ä½¿ç”¨ view (è¦æ±‚è¿ç»­) æˆ– flatten
x = x.view(B, -1)  # å¦‚æœå·²è¿ç»­
x = x.flatten(start_dim=1)  # æ›´å®‰å…¨
x = x.contiguous().view(B, -1)  # ç¡®ä¿è¿ç»­åå† view
```

### 2. è®ºæ–‡å›¾è¡¨å¯èƒ½æœ‰è¯¯
- ä½œè€…å¯èƒ½ç”»é”™äº†æ¶æ„å›¾ï¼Œ**ä»¥æ–‡å­—æè¿°å’Œå…¬å¼ä¸ºå‡†**
- å¦‚å‘ç°å›¾æ–‡ä¸ä¸€è‡´ï¼Œåœ¨æ³¨é‡Šä¸­æ ‡æ³¨

### 3. è¶…å‚æ•°é™·é˜±
- åŒºåˆ†"è®­ç»ƒæ—¶ä½¿ç”¨çš„è¶…å‚æ•°" vs "æœ€ç»ˆæŠ¥å‘Šçš„æœ€ä½³è¶…å‚æ•°"
- è®ºæ–‡å¯èƒ½æŠ¥å‘Šäº†å¤šæ¬¡å®éªŒçš„æœ€ä¼˜å€¼ï¼Œè€Œéå•æ¬¡è®­ç»ƒé…ç½®

### 4. å®˜æ–¹ä»£ç ä¸è®ºæ–‡ä¸ä¸€è‡´
- å¦‚æœè®ºæ–‡æä¾›äº†å®˜æ–¹ä»£ç ï¼Œ**ä¼˜å…ˆå‚è€ƒä»£ç å®ç°**
- åœ¨æ³¨é‡Šä¸­æ ‡æ³¨ä»£ç ä¸è®ºæ–‡æè¿°çš„å·®å¼‚

### 5. æ¡†æ¶å·®å¼‚
```python
# âš ï¸ BatchNorm momentum åœ¨ PyTorch å’Œ TensorFlow ä¸­å®šä¹‰ä¸åŒ!
# PyTorch: new_running_mean = (1 - momentum) * running_mean + momentum * batch_mean
# TensorFlow: new_running_mean = momentum * running_mean + (1 - momentum) * batch_mean
# 
# å¦‚æœè®ºæ–‡ä½¿ç”¨ TensorFlow ä¸” momentum=0.9ï¼Œåœ¨ PyTorch ä¸­åº”è®¾ä¸º 0.1
nn.BatchNorm2d(channels, momentum=0.1)  # Equivalent to TF momentum=0.9
```

### 6. åˆå§‹åŒ–å·®å¼‚
- ä¸åŒæ¡†æ¶çš„é»˜è®¤åˆå§‹åŒ–æ–¹å¼ä¸åŒ
- å¦‚è®ºæ–‡æœªæŒ‡å®šï¼Œä¼˜å…ˆä½¿ç”¨ PyTorch é»˜è®¤å€¼å¹¶æ ‡æ³¨

---

## PDF Reading Guidelines

å½“è¯»å– PDF è®ºæ–‡æ—¶ï¼Œéœ€è¦ç‰¹åˆ«å…³æ³¨ï¼š

### 1. å›¾è¡¨è§£æ
- **Figure**: ç½‘ç»œæ¶æ„å›¾ã€æ•°æ®æµå›¾ã€å®éªŒç»“æœå¯è§†åŒ–
- **Table**: è¶…å‚æ•°é…ç½®ã€å®éªŒå¯¹æ¯”ã€æ¶ˆèå®éªŒç»“æœ
- **Algorithm**: ä¼ªä»£ç ã€è®­ç»ƒæµç¨‹

### 2. å…¬å¼æå–
- ä¸»è¦å…³æ³¨ Method/Approach ç« èŠ‚çš„å…¬å¼
- æ³¨æ„å…¬å¼ç¼–å·ï¼Œä¾¿äºå¼•ç”¨
- æ£€æŸ¥ Appendix ä¸­çš„è¡¥å……å…¬å¼

### 3. å®ç°ç»†èŠ‚
- **Section 3/4 (Method)**: æ ¸å¿ƒæ¶æ„æè¿°
- **Section 5 (Experiments)**: è®­ç»ƒç»†èŠ‚ã€è¶…å‚æ•°
- **Appendix**: é¢å¤–å®ç°ç»†èŠ‚ã€å®Œæ•´é…ç½®

### 4. ä»£ç å¼•ç”¨
å¦‚æœè®ºæ–‡æä¾›äº†å®˜æ–¹ä»£ç ä»“åº“ï¼š
- ä¼˜å…ˆå‚è€ƒå®˜æ–¹å®ç°
- æ ‡æ³¨ä»£ç ç‰ˆæœ¬/commit hash
- å¯¹æ¯”è®ºæ–‡æè¿°ä¸ä»£ç å®ç°çš„å·®å¼‚

---

## Output Structure

æœ€ç»ˆè¾“å‡ºåº”åŒ…å«ä»¥ä¸‹æ–‡ä»¶ç»“æ„ï¼š

```
output/
â”œâ”€â”€ README.md           # è®ºæ–‡æ¦‚è¿°ã€ä½¿ç”¨è¯´æ˜
â”œâ”€â”€ model.py            # ä¸»æ¨¡å‹å®ç°
â”œâ”€â”€ modules/            # å­æ¨¡å—
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ attention.py    # æ³¨æ„åŠ›æ¨¡å—
â”‚   â”œâ”€â”€ backbone.py     # éª¨å¹²ç½‘ç»œ
â”‚   â””â”€â”€ head.py         # è¾“å‡ºå¤´
â”œâ”€â”€ config.py           # é…ç½®æ–‡ä»¶
â””â”€â”€ requirements.txt    # ä¾èµ–åˆ—è¡¨
```

---

## Constraints

### å·¥å…·æ ˆ
- Python 3.10+
- PyTorch 2.0+
- Mermaid (for diagrams)
- einops (optional, for tensor operations)
- timm (optional, for pretrained backbones)

### è¯­è¨€è§„èŒƒ
- **åˆ†æä¸è§£é‡Š**: ä½¿ç”¨ç®€ä½“ä¸­æ–‡
- **ä»£ç æ³¨é‡Š**: ä½¿ç”¨è‹±æ–‡
- **å˜é‡å‘½å**: ä½¿ç”¨è‹±æ–‡ï¼Œéµå¾ª PEP 8

---

## Verification Checklist

åœ¨å®Œæˆå¤ç°åï¼Œæ£€æŸ¥ä»¥ä¸‹é¡¹ç›®ï¼š

**Phase 1 - è®ºæ–‡è§£æ„**:
- [ ] æ ¸å¿ƒé—®é¢˜å’Œåˆ›æ–°ç‚¹å·²æ˜ç¡®
- [ ] æ‰€æœ‰å…³é”®å…¬å¼å·²æå–å¹¶è§£é‡Š
- [ ] è¶…å‚æ•°é…ç½®å®Œæ•´ï¼ŒæœªæŒ‡å®šé¡¹å·²æŒ‰è§„èŒƒæ ‡æ³¨

**Phase 2 - æ¶æ„å¯è§†åŒ–**:
- [ ] Mermaid æµç¨‹å›¾åŒ…å«æ‰€æœ‰ä¸»è¦ç»„ä»¶
- [ ] æ¯ä¸ªèŠ‚ç‚¹æ ‡æ³¨äº† Tensor Shape (ä½¿ç”¨æ ‡å‡†ç¬¦å·)
- [ ] ä½¿ç”¨é¢œè‰²åŒºåˆ†ä¸åŒç±»å‹ç»„ä»¶
- [ ] æ·±å±‚ç½‘ç»œä½¿ç”¨æ¨¡å—çº§ç²’åº¦

**Phase 3 - ä»£ç å®ç°**:
- [ ] ç»§æ‰¿ `nn.Module`
- [ ] å®Œæ•´çš„ç±»å‹æç¤º
- [ ] æ¯å±‚æ“ä½œåæœ‰ç»´åº¦æ³¨é‡Š (ä½¿ç”¨æ ‡å‡†ç¬¦å·)
- [ ] å­æ¨¡å—å·²æ‹†åˆ†
- [ ] åŒ…å« `_init_weights` æ–¹æ³•
- [ ] åŒ…å« `set_seed` å‡½æ•°
- [ ] æœªæŒ‡å®šå‚æ•°å·²æŒ‰ Protocol æ ‡æ³¨

**éªŒè¯æµ‹è¯•**:
- [ ] éšæœº Tensor å‰å‘ä¼ æ’­æ— æŠ¥é”™
- [ ] è¾“å‡º Shape ä¸é¢„æœŸä¸€è‡´
- [ ] å‚æ•°é‡ä¸è®ºæ–‡ä¸€è‡´ (å¦‚æœ‰æŠ¥å‘Šï¼Œå…è®¸ 1% è¯¯å·®)
- [ ] æ¢¯åº¦æµæ£€æŸ¥é€šè¿‡
- [ ] è¾“å‡ºæ•°å€¼èŒƒå›´åˆç†

**å¸¸è§é™·é˜±æ£€æŸ¥**:
- [ ] å·²æ£€æŸ¥ reshape/view çš„ä½¿ç”¨
- [ ] å·²æ ¸å¯¹è®ºæ–‡å›¾è¡¨ä¸æ–‡å­—æè¿°
- [ ] å·²æ³¨æ„æ¡†æ¶å·®å¼‚ (å¦‚ BatchNorm momentum)
